{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import nltk\n",
    "\n",
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"data/data.csv\", index_col=0)\n",
    "y = pd.read_csv(\"data/labels.csv\", index_col=0)\n",
    "\n",
    "# Drop NAs\n",
    "df.dropna(how='all', inplace=True)\n",
    "y.dropna(how='all', inplace=True)\n",
    "\n",
    "# The labeled data\n",
    "X = df.loc[y.index]\n",
    "\n",
    "# The unlabeled data\n",
    "unlabeled = df.loc[~df.index.isin(y.index)]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 training examples and 37 validation examples.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{X_train.shape[0]} training examples and {X_test.shape[0]} validation examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "import string\n",
    "\n",
    "import scipy \n",
    "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transforms input data by using NLTK tokenization, lemmatization, and\n",
    "    other normalization and filtering techniques.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stopwords=None, punct=None, lower=True, strip=True):\n",
    "        \"\"\"\n",
    "        Instantiates the preprocessor, which make load corpora, models, or do\n",
    "        other time-intenstive NLTK data loading.\n",
    "        \"\"\"\n",
    "        self.lower      = lower\n",
    "        self.strip      = strip\n",
    "        self.stopwords  = stopwords or set(sw.words('english'))\n",
    "        self.punct      = punct or set(string.punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit simply returns self, no other information is needed.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        \"\"\"\n",
    "        No inverse transformation\n",
    "        \"\"\"\n",
    "        return X\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Actually runs the preprocessing on each document.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            list(self.tokenize(doc)) for doc in X\n",
    "        ]\n",
    "\n",
    "    def tokenize(self, document):\n",
    "        \"\"\"\n",
    "        Returns a normalized, lemmatized list of tokens from a document by\n",
    "        applying segmentation (breaking into sentences), then word/punctuation\n",
    "        tokenization, and finally part of speech tagging. It uses the part of\n",
    "        speech tags to look up the lemma in WordNet, and returns the lowercase\n",
    "        version of all the words, removing stopwords and punctuation.\n",
    "        \"\"\"\n",
    "        # Break the document into sentences\n",
    "        for sent in sent_tokenize(document):\n",
    "            # Break the sentence into part of speech tagged tokens\n",
    "            for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "                # Apply preprocessing to the token\n",
    "                token = token.lower() if self.lower else token\n",
    "                token = token.strip() if self.strip else token\n",
    "                token = token.strip('_') if self.strip else token\n",
    "                token = token.strip('*') if self.strip else token\n",
    "\n",
    "                # If punctuation or stopword, ignore token and continue\n",
    "                if token in self.stopwords or all(char in self.punct for char in token):\n",
    "                    continue\n",
    "\n",
    "                # Lemmatize the token and yield\n",
    "                lemma = self.lemmatize(token, tag)\n",
    "                yield lemma\n",
    "\n",
    "    def lemmatize(self, token, tag):\n",
    "        \"\"\"\n",
    "        Converts the Penn Treebank tag to a WordNet POS tag, then uses that\n",
    "        tag to perform much more accurate WordNet lemmatization.\n",
    "        \"\"\"\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#\n",
    "# Tokenization and stemming\n",
    "#\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "# Note: punctuation is completely ignored and always treated as a token separator by CountVectorizer\n",
    "class StemmedCountVectorizer(CountVectorizer): \n",
    "    \"\"\"Source: building Machine Learning Systems with Python, 2nd ed.\"\"\"\n",
    "\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ............. (step 1 of 2) Processing union, total=   3.1s\n",
      "[Pipeline] ............... (step 2 of 2) Processing svc, total=   0.0s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "stemmedVectorizer = StemmedCountVectorizer(lowercase=True,\n",
    "                                           stop_words='english',\n",
    "                                           analyzer='word',\n",
    "                                           # tokenizer=,\n",
    "                                           ngram_range=(2, 2),\n",
    "                                           )\n",
    "\n",
    "def identity(arg):\n",
    "    \"\"\"\n",
    "    Simple identity function works as a passthrough.\n",
    "    \"\"\"\n",
    "    return arg\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    # Use ColumnTransformer to combine the features from subject and body\n",
    "    ('union', ColumnTransformer(\n",
    "        [\n",
    "            # budget column\n",
    "            ('budget', StandardScaler(), ['budget']),\n",
    "\n",
    "            # title column\n",
    "            ('title_vec', Pipeline([\n",
    "                ('preprocessor', NLTKPreprocessor()),\n",
    "                ('tfidf', TfidfVectorizer(tokenizer=identity, preprocessor=None, lowercase=False, use_idf=True)),\n",
    "            ]), 'title'),\n",
    "\n",
    "            # snippet column\n",
    "            ('snippet_vec', Pipeline([\n",
    "                ('preprocessor', NLTKPreprocessor()),\n",
    "                ('tfidf', TfidfVectorizer(tokenizer=identity, preprocessor=None, lowercase=False, use_idf=True)),\n",
    "                ('best', TruncatedSVD(n_components=50)),\n",
    "            ]), 'snippet'),\n",
    "        ]\n",
    "    )),\n",
    "\n",
    "    # Classifier\n",
    "    ('svc', LinearSVC(dual=False)),\n",
    "], verbose=True)\n",
    "\n",
    "text_clf = pipeline.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bad</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.79</td>\n",
       "      <td>18.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Good</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.67</td>\n",
       "      <td>13.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maybe</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.25</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.57</td>\n",
       "      <td>37.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.66</td>\n",
       "      <td>37.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision  recall  f1-score  support\n",
       "Bad                0.87    0.72      0.79    18.00\n",
       "Good               0.55    0.85      0.67    13.00\n",
       "Maybe              0.50    0.17      0.25     6.00\n",
       "accuracy           0.68    0.68      0.68     0.68\n",
       "macro avg          0.64    0.58      0.57    37.00\n",
       "weighted avg       0.70    0.68      0.66    37.00"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = text_clf.predict(X_test)\n",
    "pd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).round(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\toolkits.win\\Miniconda3\\envs\\dlwin36\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:430: FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit. This will fail from v0.24.\n",
      "  FutureWarning)\n",
      "C:\\toolkits.win\\Miniconda3\\envs\\dlwin36\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:430: FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit. This will fail from v0.24.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Predict new jobs\n",
    "#\n",
    "unlabeled = unlabeled.assign(predicted=text_clf.predict(unlabeled))\n",
    "\n",
    "# SVM probabilities\n",
    "p = np.array(text_clf.decision_function(unlabeled)) # decision is a voting function\n",
    "prob = np.exp(p) / np.sum(np.exp(p), axis=1).reshape(-1,1) # softmax after the voting\n",
    "# https://stackoverflow.com/questions/49507066/predict-probabilities-using-svm\n",
    "unlabeled = unlabeled.assign(probability=prob.max(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>~0181deb6b4e8d0b731</th>\n",
       "      <td>Good</td>\n",
       "      <td>0.425184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>~01b2f8a160f213b8ba</th>\n",
       "      <td>Bad</td>\n",
       "      <td>0.374196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>~010e8ea5d81bcc954f</th>\n",
       "      <td>Good</td>\n",
       "      <td>0.458577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>~01ae25809be274c3f5</th>\n",
       "      <td>Good</td>\n",
       "      <td>0.492270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>~018d743d56d6cdd02b</th>\n",
       "      <td>Good</td>\n",
       "      <td>0.405375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>~010e005dfb4173f499</th>\n",
       "      <td>Bad</td>\n",
       "      <td>0.571607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>~016485582b73c87cae</th>\n",
       "      <td>Bad</td>\n",
       "      <td>0.796040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>~01948849d5a81a5244</th>\n",
       "      <td>Bad</td>\n",
       "      <td>0.544825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>~01aba1d62da0dccca6</th>\n",
       "      <td>Bad</td>\n",
       "      <td>0.540915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>~01a33b4bfd5acddf7e</th>\n",
       "      <td>Good</td>\n",
       "      <td>0.604352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2754 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    predicted  probability\n",
       "id                                        \n",
       "~0181deb6b4e8d0b731      Good     0.425184\n",
       "~01b2f8a160f213b8ba       Bad     0.374196\n",
       "~010e8ea5d81bcc954f      Good     0.458577\n",
       "~01ae25809be274c3f5      Good     0.492270\n",
       "~018d743d56d6cdd02b      Good     0.405375\n",
       "...                       ...          ...\n",
       "~010e005dfb4173f499       Bad     0.571607\n",
       "~016485582b73c87cae       Bad     0.796040\n",
       "~01948849d5a81a5244       Bad     0.544825\n",
       "~01aba1d62da0dccca6       Bad     0.540915\n",
       "~01a33b4bfd5acddf7e      Good     0.604352\n",
       "\n",
       "[2754 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled.iloc[:,-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bad      1829\n",
       "Good      794\n",
       "Maybe     131\n",
       "Name: predicted, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled['predicted'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "~0181deb6b4e8d0b731                                                        Python feature engineering for quant in python\n",
       "~010e8ea5d81bcc954f                                                                    Dialogflow expert to review a flow\n",
       "~01ae25809be274c3f5                                                                     Help with the AI homework(python)\n",
       "~018d743d56d6cdd02b                                                                   ML-based combinatorial Optimization\n",
       "~01e0c4348d9b882379                                       Artificial Intelligence Real life academic project ideas needed\n",
       "                                                                      ...                                                \n",
       "~012463d73e3171ca2a                                                                                           Data Mining\n",
       "~017b117a8d6bee91c2                                                       WordPress / Data Acquisition & Import Assistant\n",
       "~01b18bd4e66ee01418    Python Programmer for complex project:  I need a programmer to develop and deploy a python script.\n",
       "~011c96e9d0cd6be2df                                                                 Install python opensource application\n",
       "~01a33b4bfd5acddf7e                                                                 AR/Computer Vision - Body Measurement\n",
       "Name: title, Length: 794, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled[unlabeled['predicted'] == \"Good\"]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    # 'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    # 'tfidf__use_idf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__max_iter': (20,),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    # 'clf__max_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.6 64-bit ('dlwin36': conda)",
   "language": "python",
   "name": "python36664bitdlwin36condaa76520963240418093dfd59726652526"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
